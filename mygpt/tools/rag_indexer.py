# rag_indexer.py
import os
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from mygpt.tools.ocr_tool import OcrTool

def build_vector_store_from_ocr(file_path: str, config, ocr_tool: OcrTool, output_path: str):
    """
    OcrToolì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ê³ ,
    ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹í•˜ì—¬ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        file_path (str): ì¸ë±ì‹±í•  PDF ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
        config (RAGConfig): ì„¤ì • ê°ì²´
        ocr_tool (OcrTool): ì´ˆê¸°í™”ëœ OcrTool ì¸ìŠ¤í„´ìŠ¤
    """
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    os.makedirs(output_path, exist_ok=True)

    print(f"ğŸ”¬ 1. OCR Toolë¡œ '{os.path.basename(file_path)}' íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    # ocr_toolì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    markdown_text = ocr_tool.get_markdown_from_file(file_path)
    
    # ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ì°¨ ì²­í‚¹
    headers_to_split_on = [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    md_header_splits = markdown_splitter.split_text(markdown_text)

    print(f"ğŸ§± 2. êµ¬ì¡°ì  ì²­í‚¹(Markdown Splitting) ì§„í–‰ ì¤‘...")
    # 1ì°¨ ì²­í‚¹ëœ ê²°ê³¼ê°€ ë„ˆë¬´ í´ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 2ì°¨ ì¬ê·€ì  ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(md_header_splits)

    # ê° ì²­í¬ì— ì›ë³¸ íŒŒì¼ëª… ë©”íƒ€ë°ì´í„° ì¶”ê°€
    for chunk in chunks:
        chunk.metadata["source"] = os.path.basename(file_path)

    print(f"ğŸ“Š ì´ {len(chunks)}ê°œì˜ êµ¬ì¡° ê¸°ë°˜ ì²­í¬ ìƒì„± ì™„ë£Œ.")
    
    print("ğŸ§  3. ì„ë² ë”© ëª¨ë¸ ë¡œë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
    embeddings = HuggingFaceEmbeddings(
        model_name=config.EMBEDDING_MODEL,
        model_kwargs=config.MODEL_KWARGS,
        encode_kwargs=config.ENCODE_KWARGS
    )
    vector_store = FAISS.from_documents(chunks, embeddings)

    vector_store.save_local(output_path)
    
    with open(os.path.join(output_path, "chunks.pkl"), "wb") as f:
        pickle.dump(chunks, f)

    print(f"âœ… ë²¡í„° ì €ì¥ì†Œì™€ ì²­í¬ ë°ì´í„°ê°€ '{output_path}' ê²½ë¡œì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
