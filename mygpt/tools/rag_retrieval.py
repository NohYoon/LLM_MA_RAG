# rag_retriever.py
import pickle
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.retrievers.document_compressors import DocumentCompressorPipeline
from langchain_community.document_transformers import LongContextReorder

class AdvancedQueryPipeline:
    def __init__(self, config):
        print("ğŸ” 1. ê³ ê¸‰ ì¿¼ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        self.config = config

        # --- ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ---
        embeddings = HuggingFaceEmbeddings(
            model_name=config.EMBEDDING_MODEL,
            model_kwargs=config.MODEL_KWARGS,
            encode_kwargs=config.ENCODE_KWARGS
        )
        vector_store = FAISS.load_local(
            config.VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True
        )
        faiss_retriever = vector_store.as_retriever(search_kwargs={"k": 10})

        # --- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(Hybrid Search) ì„¤ì • ---
        print("ğŸ¤ 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25 + FAISS) ì„¤ì • ì¤‘...")
        with open(f"{config.VECTOR_STORE_PATH}/chunks.pkl", "rb") as f:
            chunks = pickle.load(f)
        bm25_retriever = BM25Retriever.from_documents(chunks)
        bm25_retriever.k = 10

        self.hybrid_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
        )

        # --- LLMì„ ì´ìš©í•œ ì¿¼ë¦¬ ë³€í™˜(Multi-Query) ì„¤ì • ---
        print("ğŸ”„ 3. ì¿¼ë¦¬ ë³€í™˜(Multi-Query) ì„¤ì • ì¤‘...")
        llm = ChatOpenAI(model=config.LLM_MODEL_NAME, temperature=0)
        self.multiquery_retriever = MultiQueryRetriever.from_llm(
            retriever=self.hybrid_retriever, llm=llm
        )

        # --- Reranker ë° í›„ì²˜ë¦¬(Reordering) ì„¤ì • ---
        print("âœ¨ 4. Reranker ë° í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì¤‘...")
        reranker = HuggingFaceCrossEncoder(model_name=config.RERANKER_MODEL)
        reordering = LongContextReorder()
        
        compressor = CrossEncoderReranker(model=reranker, top_n=5)
        # ì••ì¶• íŒŒì´í”„ë¼ì¸: Reranker ì‹¤í–‰ í›„ -> LongContextReorder ì‹¤í–‰
        pipeline_compressor = DocumentCompressorPipeline(
            transformers=[compressor, reordering]
        )
        
        # ìµœì¢… Retriever: ì¿¼ë¦¬ ë³€í™˜ -> í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ -> Reranking -> Reordering
        self.final_retriever = ContextualCompressionRetriever(
            base_compressor=pipeline_compressor, base_retriever=self.multiquery_retriever
        )
        print("âœ… ê³ ê¸‰ ì¿¼ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def query(self, question: str):
        print(f"\nğŸ’¬ ì§ˆë¬¸: '{question}'")
        final_docs = self.final_retriever.invoke(question)
        print(f"ğŸ¯ ì´ {len(final_docs)}ê°œì˜ ìµœì¢… ë¬¸ì„œ ë°˜í™˜.")
        return final_docs